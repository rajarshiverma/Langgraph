{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import TypedDict, List\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, START, END\nfrom dotenv import load_dotenv # used to store secret stuff like API keys or configuration values\n\nload_dotenv()\n\nclass AgentState(TypedDict):\n    messages: List[HumanMessage]\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\ndef process(state: AgentState) -> AgentState:\n    response = llm.invoke(state[\"messages\"])\n    print(f\"\\nAI: {response.content}\")\n    return state\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"process\", process)\ngraph.add_edge(START, \"process\")\ngraph.add_edge(\"process\", END) \nagent = graph.compile()\n\nuser_input = input(\"Enter: \")\nwhile user_input != \"exit\":\n    agent.invoke({\"messages\": [HumanMessage(content=user_input)]})\n    user_input = input(\"Enter: \")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom typing import TypedDict, List, Union\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, START, END\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass AgentState(TypedDict):\n    messages: List[Union[HumanMessage, AIMessage]]\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\ndef process(state: AgentState) -> AgentState:\n    \"\"\"This node will solve the request you input\"\"\"\n    response = llm.invoke(state[\"messages\"])\n\n    state[\"messages\"].append(AIMessage(content=response.content)) \n    print(f\"\\nAI: {response.content}\")\n    print(\"CURRENT STATE: \", state[\"messages\"])\n\n    return state\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"process\", process)\ngraph.add_edge(START, \"process\")\ngraph.add_edge(\"process\", END) \nagent = graph.compile()\n\n\nconversation_history = []\n\nuser_input = input(\"Enter: \")\nwhile user_input != \"exit\":\n    conversation_history.append(HumanMessage(content=user_input))\n    result = agent.invoke({\"messages\": conversation_history})\n    conversation_history = result[\"messages\"]\n    user_input = input(\"Enter: \")\n\n\nwith open(\"logging.txt\", \"w\") as file:\n    file.write(\"Your Conversation Log:\\n\")\n    \n    for message in conversation_history:\n        if isinstance(message, HumanMessage):\n            file.write(f\"You: {message.content}\\n\")\n        elif isinstance(message, AIMessage):\n            file.write(f\"AI: {message.content}\\n\\n\")\n    file.write(\"End of Conversation\")\n\nprint(\"Conversation saved to logging.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Annotated, Sequence, TypedDict\nfrom dotenv import load_dotenv  \nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import ToolNode\n\nload_dotenv()\n\n# This is the global variable to store document content\ndocument_content = \"\"\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n\n@tool\ndef update(content: str) -> str:\n    \"\"\"Updates the document with the provided content.\"\"\"\n    global document_content\n    document_content = content\n    return f\"Document has been updated successfully! The current content is:\\n{document_content}\"\n\n\n@tool\ndef save(filename: str) -> str:\n    \"\"\"Save the current document to a text file and finish the process.\n    \n    Args:\n        filename: Name for the text file.\n    \"\"\"\n\n    global document_content\n\n    if not filename.endswith('.txt'):\n        filename = f\"{filename}.txt\"\n\n\n    try:\n        with open(filename, 'w') as file:\n            file.write(document_content)\n        print(f\"\\nðŸ’¾ Document has been saved to: {filename}\")\n        return f\"Document has been saved successfully to '{filename}'.\"\n    \n    except Exception as e:\n        return f\"Error saving document: {str(e)}\"\n    \n\ntools = [update, save]\n\nmodel = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)\n\ndef our_agent(state: AgentState) -> AgentState:\n    system_prompt = SystemMessage(content=f\"\"\"\n    You are Drafter, a helpful writing assistant. You are going to help the user update and modify documents.\n    \n    - If the user wants to update or modify content, use the 'update' tool with the complete updated content.\n    - If the user wants to save and finish, you need to use the 'save' tool.\n    - Make sure to always show the current document state after modifications.\n    \n    The current document content is:{document_content}\n    \"\"\")\n\n    if not state[\"messages\"]:\n        user_input = \"I'm ready to help you update a document. What would you like to create?\"\n        user_message = HumanMessage(content=user_input)\n\n    else:\n        user_input = input(\"\\nWhat would you like to do with the document? \")\n        print(f\"\\nðŸ‘¤ USER: {user_input}\")\n        user_message = HumanMessage(content=user_input)\n\n    all_messages = [system_prompt] + list(state[\"messages\"]) + [user_message]\n\n    response = model.invoke(all_messages)\n\n    print(f\"\\nðŸ¤– AI: {response.content}\")\n    if hasattr(response, \"tool_calls\") and response.tool_calls:\n        print(f\"ðŸ”§ USING TOOLS: {[tc['name'] for tc in response.tool_calls]}\")\n\n    return {\"messages\": list(state[\"messages\"]) + [user_message, response]}\n\n\ndef should_continue(state: AgentState) -> str:\n    \"\"\"Determine if we should continue or end the conversation.\"\"\"\n\n    messages = state[\"messages\"]\n    \n    if not messages:\n        return \"continue\"\n    \n    # This looks for the most recent tool message....\n    for message in reversed(messages):\n        # ... and checks if this is a ToolMessage resulting from save\n        if (isinstance(message, ToolMessage) and \n            \"saved\" in message.content.lower() and\n            \"document\" in message.content.lower()):\n            return \"end\" # goes to the end edge which leads to the endpoint\n        \n    return \"continue\"\n\ndef print_messages(messages):\n    \"\"\"Function I made to print the messages in a more readable format\"\"\"\n    if not messages:\n        return\n    \n    for message in messages[-3:]:\n        if isinstance(message, ToolMessage):\n            print(f\"\\nðŸ› ï¸ TOOL RESULT: {message.content}\")\n\n\ngraph = StateGraph(AgentState)\n\ngraph.add_node(\"agent\", our_agent)\ngraph.add_node(\"tools\", ToolNode(tools))\n\ngraph.set_entry_point(\"agent\")\n\ngraph.add_edge(\"agent\", \"tools\")\n\n\ngraph.add_conditional_edges(\n    \"tools\",\n    should_continue,\n    {\n        \"continue\": \"agent\",\n        \"end\": END,\n    },\n)\n\napp = graph.compile()\n\ndef run_document_agent():\n    print(\"\\n ===== DRAFTER =====\")\n    \n    state = {\"messages\": []}\n    \n    for step in app.stream(state, stream_mode=\"values\"):\n        if \"messages\" in step:\n            print_messages(step[\"messages\"])\n    \n    print(\"\\n ===== DRAFTER FINISHED =====\")\n\nif __name__ == \"__main__\":\n    run_document_agent()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import Annotated, Sequence, TypedDict\nfrom dotenv import load_dotenv  \nfrom langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph\nfrom langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id\nfrom langchain_core.messages import SystemMessage # Message for providing instructions to the LLM\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import ToolNode\n\n\nload_dotenv()\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n\n@tool\ndef add(a: int, b:int):\n    \"\"\"This is an addition function that adds 2 numbers together\"\"\"\n\n    return a + b \n\n@tool\ndef subtract(a: int, b: int):\n    \"\"\"Subtraction function\"\"\"\n    return a - b\n\n@tool\ndef multiply(a: int, b: int):\n    \"\"\"Multiplication function\"\"\"\n    return a * b\n\ntools = [add, subtract, multiply]\n\nmodel = ChatOpenAI(model = \"gpt-4o\").bind_tools(tools)\n\n\ndef model_call(state:AgentState) -> AgentState:\n    system_prompt = SystemMessage(content=\n        \"You are my AI assistant, please answer my query to the best of your ability.\"\n    )\n    response = model.invoke([system_prompt] + state[\"messages\"])\n    return {\"messages\": [response]}\n\n\ndef should_continue(state: AgentState): \n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls: \n        return \"end\"\n    else:\n        return \"continue\"\n    \n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"our_agent\", model_call)\n\n\ntool_node = ToolNode(tools=tools)\ngraph.add_node(\"tools\", tool_node)\n\ngraph.set_entry_point(\"our_agent\")\n\ngraph.add_conditional_edges(\n    \"our_agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"end\": END,\n    },\n)\n\ngraph.add_edge(\"tools\", \"our_agent\")\n\napp = graph.compile()\n\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\ninputs = {\"messages\": [(\"user\", \"Add 40 + 12 and then multiply the result by 6. Also tell me a joke please.\")]}\nprint_stream(app.stream(inputs, stream_mode=\"values\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}